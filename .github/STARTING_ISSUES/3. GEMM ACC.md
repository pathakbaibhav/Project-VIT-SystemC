# 3. GEMM ACC Without Tiling

This issue develops the initial version of the HW GEMM accelerator and validates it using SWEmu driving the ACC without tiling. 

Use the FP GEMM code developed in lab 1 and create a GEMM accelerator out of it. Validate timing. 

## 1. Implement MMR read and write functionality 

1. The MMR defintion shows that memory space is contigous. With this, instead of defining each MMR variable individually, we can capture those in one structure (which will also have contiguous memory). Define a top-level `tRegs` structure that contains all MMRs (but not the scratch pad). Create a structure definition for the instruction descriptor (`tDescr`) containing the individual descriptor elements. Then, instanciate an array of descriptors `tDescr descrA[DESCR_MAX]` as part of `tRegs` with `#define DESCR_MAX 1` dimesions. Having multiple descriptors is future work. 

2. Implement read/write access to the MMRs in b_transport. Hint use a memcpy approach based on the base address of `tRegs` and the offset to avoid having to manually implement each register read/write individually (a similar approach is used in [demo-dma.cc](https://github.com/neu-ece-7368/cosim-dma-demo/blob/main/hw/demo-dma.cc)). Only the CSR needs an additional treatment for starting processing. 

4. Create scratchpad memory. The scratchpad is modeled as a simple array local to the GEMM processor. This correlates to the actual implementation on the FPGA to using the BRAM. The Zynq 7020 has 4.9Mb (Mega bit) BRAM. But we assume only 512kByte of those can be used for the scratch pad. The remaining BRAMs are used for internal buffers (which we don't model). Implement the same read/write approach as for the vectors in the vector processor. 

   The unified scratchpad will hold all input / output data (combining MA, MB, MC). Since it is unified, varying sizes of matrices can be computed. The SW driver defines where to place the matrices. This gives the flexibility to eventually allow more than one data set to be loaded in the GEMM processor allowing for overlapped communication and computation. 

5. Validate write and read of MMRs and scratch pad with SWEMU. 

## 2. GEMM Processing and Delay

1. Copy the `gemm_nn` implementation from SWEmu into the gemm_processor. Call gemm_nn from within the `thread_process` with the inputs / outputs / dimensions defined in the descriptor. Trigger processing when LSB in CSR is set and clear CSR LSB when done. 

2. Compute the processing delay based on actual dimensions. The assumptions are the same as in the separate assignment: 80% of 196 MAC units @ 300MHz produce each 1 MAC result per cycle. 

3. Change the implementation of `acc_driver()` to interface with the GEMM ACC. It should first copy the matrices into the ACC's scratch pad. Start with MA at offset 0 and increase contiguously from there. After that write the descriptor with the actual addresses and lengths and trigger processing. Run the small test cases to validate correctness (comment out large and darknet as those require tiling). 


## 3. Dimensioning Background

As discussed in the separate Quiz.  Zynq 7020: 

 - 220 DSP SLICES (18x25bit MACs) up to 741MHz [Manual](https://www.xilinx.com/content/dam/xilinx/support/documents/data_sheets/ds190-Zynq-7000-Overview.pdf), "DSP Signal Processing - DSP Slice"

 - Make function for calculating the processing delay asumuing
   - compute the number of MACs in GEMM call (dim_M, dim_N, dim_K)
   - How long would it take to compute the MACs on  80% DSP slice utiliziation @ 400MHz (timing closure problems)? Assume that an micro architecture can be found that perfectly parallelizes the workload. 

- From BRAM not all can be used as general buffer:
    - internal storage in GEMM compute fabric to allow for buffering and rate adaptation
    - e.g. sqrt(n DSP slices) * 16bit * (SYST_FIFO_ROW + SYST_FIFO_COL) (FIFOs on every systolic array input, maybe different by row and column)
    - remainder is the input / output buffer MMRs
      -> 14x14 square systolic array (196)



    - MMRs
        - MMR_CSR (start, irq enable/status, instruction nr)
        - MMR_DESCR_NR ()
            - currently executed descriptor nr: 1 is first instruction, 0 is idle
        - Istruction Descriptor, separate MMRs for "instructions", make N_INST instructions (starting with 1)
            - MA_START, MB_START, MC_START, dim_M, dim_N, dim_K, InstNext
                - Matrix start addreses are relative to begin of MMR_DATA
                - InstNext is 0, unused for now.
        - Implementation hint for MMRs
            - define a structure for descriptor, create array of descriptors with `#define DESCR_MAX 1` dimesions, use memcpy approach to read / write (similar to data)
            - a later lab has option to extend to multiple descriptors

